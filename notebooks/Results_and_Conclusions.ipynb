{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JsjZgJ6urXT"
   },
   "source": [
    "# Results and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPj3fMCCsa8G"
   },
   "source": [
    "<a id ='TOC'></a>\n",
    "### Table of Contents\n",
    "   1. [Summary of Results](#Summary-of-Results) <br/>\n",
    "   2. [Noteworthy Findings](#Noteworthy-Findings) <br/>\n",
    "   3. [Challenges](#Challenges) <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HAZS81cJHqAn"
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwjccZkUpxH7"
   },
   "source": [
    "We did not have a lot of data to train our models, good labelling mechanics to label our data, and most of the users were not bots. However, we were able to train models that performs better than random guessing using the base rate for bots vs non-bots in our samples.\n",
    "\n",
    "**Our best model reached an accuracy of 94.4%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EsfPscxDsa-g"
   },
   "source": [
    "[Back to TOC](#TOC) <br/>\n",
    "<a id ='Summary-of-Results'></a>\n",
    "## 1. Summary of Results\n",
    "\n",
    "#### Model comparison and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ayJqrbaopxH7",
    "outputId": "4221d32b-c517-4ff1-98ed-547c8955e5e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bl</th>\n",
       "      <th>ols</th>\n",
       "      <th>ridge</th>\n",
       "      <th>lasso</th>\n",
       "      <th>lm</th>\n",
       "      <th>lm_cv3</th>\n",
       "      <th>lm_poly3</th>\n",
       "      <th>knn_17</th>\n",
       "      <th>dtc</th>\n",
       "      <th>rf</th>\n",
       "      <th>adaboost</th>\n",
       "      <th>svm_poly_c1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.917692</td>\n",
       "      <td>0.913907</td>\n",
       "      <td>0.919584</td>\n",
       "      <td>0.917692</td>\n",
       "      <td>0.914853</td>\n",
       "      <td>0.919584</td>\n",
       "      <td>0.934721</td>\n",
       "      <td>0.927152</td>\n",
       "      <td>0.928098</td>\n",
       "      <td>0.936613</td>\n",
       "      <td>0.944182</td>\n",
       "      <td>0.932829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.917324</td>\n",
       "      <td>0.918586</td>\n",
       "      <td>0.919217</td>\n",
       "      <td>0.917955</td>\n",
       "      <td>0.925529</td>\n",
       "      <td>0.925844</td>\n",
       "      <td>0.982644</td>\n",
       "      <td>0.930577</td>\n",
       "      <td>0.966867</td>\n",
       "      <td>0.957400</td>\n",
       "      <td>0.956453</td>\n",
       "      <td>0.949826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             bl       ols     ridge     lasso        lm    lm_cv3  lm_poly3    knn_17       dtc        rf  adaboost  svm_poly_c1\n",
       "test   0.917692  0.913907  0.919584  0.917692  0.914853  0.919584  0.934721  0.927152  0.928098  0.936613  0.944182     0.932829\n",
       "train  0.917324  0.918586  0.919217  0.917955  0.925529  0.925844  0.982644  0.930577  0.966867  0.957400  0.956453     0.949826"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = pd.read_json('acc.json')\n",
    "display(acc)\n",
    "with open('models.pickle', 'rb') as handle:\n",
    "    models_list = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmZfKntssa-h"
   },
   "source": [
    "[Back to TOC](#TOC) <br/>\n",
    "<a id ='Noteworthy-Findings'></a>\n",
    "## 2.  Noteworthy Findings\n",
    "\n",
    "### Botometer Label Accuracy<br/>\n",
    "\n",
    "We noticed that botometer scores were not always accurate. We were able to improve the botometer score prediction for actual bot / non-bot detection using a simple extended model. As we only have a small number of manually verified samples, the results we got was not perfect. However, there is an improvement could be achieved using this technique with a larger manually verified user dataset. <br/><br/> A generalization of this technique / approch is that it allow us to train a model using a large dataset with imperfect labels, use those predictions to train a model on a smaller dataset with better labels. This ensembled model could achieve an improvement on prediction than using the large dataset or the small datset alone. <br/><br/> We were able to get some promising initial results from an unsupervised KMeans model, which we could investigate further to see if we could avoid the need for using botometer labels. Similar to botometer score, KMeans clustering could also be used to a smaller dataset with manually verified labels to create an ensemble model.\n",
    "\n",
    "### Class and Imbalance <br/>\n",
    "\n",
    "Among all the users, vastly majority of them were labeled as real users by botometer, which casued class imbalance in our data and potentially could result in very high accuracy (even if the model may not be that good). We tried to resolve this issue by stratify our data by botometer results, so similar proportion of bots were presented in trian and test set. <br/> <br/>\n",
    "One thing we could have done, however, is use sampling to reach 50/50 balance. \n",
    "\n",
    "### Weights<br/>\n",
    "\n",
    "Another technique we could have done is to change loss functions to weight errors on bots higher. Similar to fraud detection in practive, we would want to make sure we do not miss any fraud (bots, in our case) as we can always verify fraud/non-fraud (bots/non-bots) with actual legit users (non-bot actual users), and we will get feedback. However, it would be very difficult to do the other way around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RH7yCarQpxH9"
   },
   "source": [
    "[Back to TOC](#TOC) <br/>\n",
    "<a id ='Challenges'></a>\n",
    "## 3.  Challenges\n",
    "\n",
    "We have learned a lot during the project, especially on how to get data and performing feature engineering, which took up most of our time and much longer than we anticipated. This is mostly due to the following challenges that we have encountered during the process:\n",
    "\n",
    "### **Memory** <br/>\n",
    "the 6000 * 200 tweets ended up to be a file of almost 7 GB. While we have access to computer with 48 Gb memory, it is still fail to load data some time. Not to mention that it becomes very challneging to run on regular PCs. This could potentially be resolved by only reading the json features that we are interested in. In that case, only part of the files will be read and it is easier for computers to handle. The memory issue is also the result of panda dataframe inefficiency and bad coding habbit (e.g. keep copying files without deleting them). \n",
    "\n",
    "### **Downloading Data with Error** <br/>\n",
    "One common thing we have encoutered quite often during the project is not except errors (tweepy errors, user_timeline errors, etc.), especially when running api. This often leads to a break with only one error and made the data collecting process longer than we expected.\n",
    "\n",
    "### **API Rate Limits** <br/>\n",
    "Collecting many tweets / botometer scores have been time consuming due to API rate limits (both from twitter and botometer). However, we also found that some API pricing could be quite affordable. Regarding the time a paid API will save on a project, we would think of this option next time.\n",
    "\n",
    "### **Data Cleaning**<br/>\n",
    "Data cleaning has been very challenging for this project - especially given the number of features embeded in each tweet, and the large number of missing data, errors, etc. Although sometimes we tried to first test on a small dataset, new errors would often occur when we tried to load a larger set of data.\n",
    "\n",
    "### **Lack of Labelled Data**<br/>\n",
    "As we were not provided with labelled data for this project, we need to find labels by ourselves (using botometer, manual verification, etc.) in order to train and/or evaluate our success. Moreover, while self-claimed bots accounts are easy to identify, often times the bots with malicious intentions would try to pretend to be a normal user, which is franky quite difficult to tell sometimes even by going through all the tweets history and reading user profiles.\n",
    "\n",
    "### **Open Ended Challenge**<br/>\n",
    "Unlike other assignments in the course, which we were provided with identified problems where approaches are clear and straightforward, for this challenge we were given an open ended challenge. Identifying the problem and design the approach have been very interesting but also challenging.\n",
    "\n",
    "### **Feature Engineering**<br/>\n",
    "Feature engineering generated most of the predictors in the dataset we used to train models. We tried to aggregate tweet level data to account level to provide more insights for each user (e.g. more uniformed tweeting time might imply a bot). However, similar to idenfitying the problem, what features to look for, how to extract them, how to execute our plan, have been challenging and time consuming.\n",
    "\n",
    "### **DEBUGGING!**<br/>\n",
    "From code not running, to graphs do not make senses, debugging has always be one of the most challenging part of this proejct. One thing we have found helpful is to debug systematically by breaking down the chunk of code one execution at a time."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Results_and_Conclusions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
